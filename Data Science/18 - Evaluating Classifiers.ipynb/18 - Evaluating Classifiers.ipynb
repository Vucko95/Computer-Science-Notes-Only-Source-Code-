{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key task when applying a classifier is to determine how effective our classifier will be at making predictions. One way to estimate this is to divide the full dataset into two sets using a \"hold-out strategy\":\n",
    "1. *Training set*: A set of examples used to build the classification model.\n",
    "2. *Test set*: A separate set of examples that is withheld from the classifier during training, and is used afterwards to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Simple Train/Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how to evaluate classifiers in scikit-learn, we will randomly generate an artificial dataset with 400 examples described by 10 features, annotated with 2 classes: Positive (1) and Negative (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "data, target = make_hastie_10_2(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "data[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each item in this artificial dataset has a label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily randomly split the complete dataset into a training test and a test set. We will specify that 20% (0.2) of the data will be used for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set has %d examples\" % data_train.shape[0] )\n",
    "print(\"Test set has %d examples\" % data_test.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build a KNN classifier ($k=3$) as we have seen previously. Note that we only use the training data to build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(data_train, target_train)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(data_test)\n",
    "print(\"Target\",target_test)\n",
    "print(\"Predictions\",predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually comparing the target labels for the test data with our predictions can be misleading. Instead, we want to determine the extent to which the classifier made the following correct/incorrect predictions:\n",
    "- *True Positives* (TP) are those which are labeled ``1`` which are actually ``1``\n",
    "- *False Positives* (FP) are those which are labeled ``1`` which are actually ``-1``\n",
    "- *True Negatives* (TN) are those which are labeled ``-1`` which are actually ``-1``\n",
    "- *False Negatives* (FN) are those which are labeled ``-1`` which are actually ``1``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this by creating a confusion matrix for the results. The result is a NumPy matrix, with predictions on the columns and actual labels on the rows. The values correspond to:\n",
    "\n",
    "    [ [TP FN]\n",
    "    [FP TN] ]\n",
    "A perfect classifier with 100% accuracy would produce a pure diagonal matrix which would have all the test examples predicted in their correct class. In our case, we see that we have many false negatives (i.e. examples labelled -1 which are actually 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all of the scikit-learn evaluation functionality\n",
    "from sklearn.metrics import *\n",
    "# build the confusion matrix\n",
    "cm = confusion_matrix(target_test, predicted,labels=[1,-1])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overall *accuracy* score for the predictions, defined as the fraction of correct predictions, can be calculated using the below. This will return a value between 0 (completely wrong) and 1 (predictions are 100% accurate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy = %.2f\" % accuracy_score(target_test, predicted) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Measures from information retrieval (search engines) can be used in ML evaluation. Note that these are calculated with respect to a particular class (e.g. the positive class labelled as \"1\").\n",
    "- *Precision*: proportion of retrieved results that are relevant = TP/(TP+FP)\n",
    "- *Recall*: proportion of relevant results that are retrieved = TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we indicate that we are interested in the Positive class here, which is labelled as \"1\"\n",
    "print(\"Precision (Positive) = %.2f\" % precision_score(target_test, predicted, pos_label=1) )\n",
    "print(\"Recall (Positive) = %.2f\" % recall_score(target_test, predicted, pos_label=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is often a trade-off between precision and recall. We can combine precision and recall into a single score using the *F1 Measure*, which is a weighted average of the precision and recall. The F1 Measure reaches its best value at 1 and worst at 0.\n",
    "\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 (Positive) = %.2f\" % f1_score(target_test, predicted, pos_label=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly compute a summary of these statistics using scikit-learn's provided convenience function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(target_test, predicted, target_names=[\"negative\",\"positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with simply randomly splitting a dataset into two sets is that each random split might give different results. We are also ignoring a portion of your dataset. One way to address this is to use *k-fold cross-validation* to evaluate a classifier:\n",
    "1. Divide the data into k disjoint subsets - “folds” (e.g. k=5).\n",
    "2. For each of k experiments, use k-1 folds for training and the selected one fold for testing.\n",
    "3. Repeat for all k folds, average the accuracy/error rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is a relatively complex process, scikit-learn allows us to achieve this using a single command. Let's do a 2-fold cross-validation of the KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a single classifier\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "# apply 2-fold cross-validation, measuring accuracy each time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "acc_scores = cross_val_score(model, data, target, cv=2, scoring=\"accuracy\")\n",
    "print(acc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for 10-fold cross validation we get an array with 10 accuracy scores, one for each fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_scores =  cross_val_score(model, data, target, cv=10, scoring=\"accuracy\")\n",
    "print(acc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average accuracy across all folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KNN: Mean cross-validation accuracy = %.2f\" % acc_scores.mean() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this approach to compare different classifiers on the same data, such as a logistic regression classifier or a Support Vector Machine (SVM) classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model = linear_model.LogisticRegression(solver='liblinear')\n",
    "acc_scores =  cross_val_score(model, data, target, cv=10, scoring=\"accuracy\")\n",
    "print(\"Logistic Regression: Mean cross-validation accuracy = %.2f\" % acc_scores.mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(gamma='auto')\n",
    "acc_scores = cross_val_score(model, data, target, cv=10, scoring=\"accuracy\")\n",
    "print(\"SVM: Mean cross-validation accuracy = %.2f\" % acc_scores.mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

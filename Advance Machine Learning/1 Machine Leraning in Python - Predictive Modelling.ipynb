{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Machine Learning in Python - Predictive Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lots of Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build predictive models in Python we use a set of libraries that are imported here. In particular **pandas** and **sklearn** are particularly important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from IPython.display import display, HTML, Image\n",
    "import io\n",
    "from operator import itemgetter\n",
    "\n",
    "from TAS_Python_Utilities import data_viz\n",
    "from TAS_Python_Utilities import data_viz_target\n",
    "from TAS_Python_Utilities import visualize_tree\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import neighbors\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "%qtconsole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & Partition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('ACMETelephoneABT.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the distribution of the two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset[\"churn\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataset.select_dtypes(include=[np.number]).describe())\n",
    "display(dataset.select_dtypes(include=[np.object]).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_viz(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_viz_target(dataset, \"churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate the descriptive features we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = dataset[['age',\n",
    " 'income',\n",
    " 'numHandsets',  \n",
    " 'handsetAge',\n",
    " 'smartPhone',\n",
    " 'currentHandsetPrice',\n",
    " 'avgBill',\n",
    " 'avgOverBundleMins',\n",
    " 'avgRoamCalls',\n",
    " 'callMinutesChangePct',\n",
    " 'billAmountChangePct',\n",
    " 'avgReceivedMins',\n",
    " 'avgOutCalls',\n",
    " 'avgInCalls',\n",
    " 'peakOffPeakRatio',\n",
    " 'peakOffPeakRatioChangePct',\n",
    " 'avgDroppedCalls',\n",
    " 'lifeTime',\n",
    " 'newFrequentNumbers']]\n",
    "Y = dataset[\"churn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a **training set**, a **vaidation set**, and a **test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_plus_valid, X_test, y_train_plus_valid, y_test \\\n",
    "    = train_test_split(X, Y, random_state=0, \\\n",
    "                                    train_size = 0.7)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid \\\n",
    "    = train_test_split(X_train_plus_valid, \\\n",
    "                                        y_train_plus_valid, \\\n",
    "                                        random_state=0, \\\n",
    "                                        train_size = 0.5/0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Very Simple Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_tree = \\\n",
    "    tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "my_tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the decision tree so we can see what it is doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_names = list(X_train.columns)\n",
    "visualize_tree(my_tree, feature_names, fileName='dt_over.png')\n",
    "Image(filename='dt_over.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the decision tree on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_tree.predict(X_train)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = \\\n",
    "metrics.accuracy_score(y_train, y_pred) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "\n",
    "print(metrics.classification_report(y_train, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(metrics.confusion_matrix(y_train, y_pred))\n",
    "\n",
    "# Print nicer homemade confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_train, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the tree on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_tree.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_valid, y_pred) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(metrics.confusion_matrix(y_valid, y_pred))\n",
    "\n",
    "# Print nicer confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate probability score based ROC evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the validation data\n",
    "y_pred_score = my_tree.predict_proba(X_valid)\n",
    "y_pred = my_tree.predict(X_valid)\n",
    "print(metrics.roc_auc_score(y_valid, y_pred_score[:, 1]))\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_valid, y_pred_score[:, 1])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a decision tree, limiting its depth to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_tree = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=2)\n",
    "my_tree = my_tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the decision tree on the **training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_tree.predict(X_train)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_train, y_pred) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_train, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_train, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the decision tree on the **validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_tree.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_valid, y_pred) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate probability score based ROC evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the validation data\n",
    "y_pred_score = my_tree.predict_proba(X_valid)\n",
    "print(metrics.roc_auc_score(y_valid, y_pred_score[:, 1]))\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_valid, y_pred_score[:, 1])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the decision tree so we can see what it is doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualise the decision tree\n",
    "feature_names = list(X_train.columns)\n",
    "visualize_tree(my_tree, feature_names, fileName=\"dt_under.png\")\n",
    "Image(filename='dt_under.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Cross Validation Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a cross validation to perfrom an evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_tree = tree.DecisionTreeClassifier(max_depth = 12)\n",
    "scores = cross_val_score(my_tree, X_train_plus_valid, y_train_plus_valid, cv=10)\n",
    "print(scores)\n",
    "scores = pd.Series(scores)\n",
    "print(scores.mean(), \" =/- \", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Parameters Using a Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tree.fit(X_train_plus_valid, y_train_plus_valid)\n",
    "my_tree.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to using post pruning explicitly is to use a grid search through a large set of possible parameters. Here we try depths between 3 and 20 and different limits on the minimum number of samples per split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid ={'criterion': ['gini', \"entropy\"], \\\n",
    "             'max_depth': list(range(3, 20, 3)), \\\n",
    "             'min_samples_split': [50] }\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_tree = GridSearchCV(tree.DecisionTreeClassifier(), \\\n",
    "                                param_grid, cv=2, verbose = 0, \\\n",
    "                            return_train_score=True)\n",
    "my_tuned_tree.fit(X_train_plus_valid, y_train_plus_valid)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "display(my_tuned_tree.best_params_)\n",
    "display(my_tuned_tree.best_score_)\n",
    "display(my_tuned_tree.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_tuned_tree.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_tree = tree.DecisionTreeClassifier(min_samples_split=50, criterion='gini', max_depth=8)\n",
    "best_tree = best_tree.fit(X_train, y_train)\n",
    "\n",
    "# visualise the decision tree\n",
    "feature_names = list(X_train.columns)\n",
    "visualize_tree(best_tree, feature_names, 'dt_tuned.png')\n",
    "Image(filename='dt_tuned.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on a stratified test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_tuned_tree.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily use the same patterns to train other types of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do the same job with random forests\n",
    "my_model = ensemble.RandomForestClassifier(n_estimators=300, \\\n",
    "                                           max_features = 3,\\\n",
    "                                           min_samples_split=200)\n",
    "my_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the model on the **validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "print(metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate probability score based ROC evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the validation data\n",
    "y_pred_score = my_model.predict_proba(X_valid)\n",
    "print(metrics.roc_auc_score(y_valid, y_pred_score[:, 1]))\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_valid, y_pred_score[:, 1])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same job with random forests\n",
    "my_model = ensemble.BaggingClassifier(base_estimator = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf = 50), \\\n",
    "                                      n_estimators=10)\n",
    "my_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "print(metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate probability score based ROC evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the validation data\n",
    "y_pred_score = my_model.predict_proba(X_valid)\n",
    "print(metrics.roc_auc_score(y_valid, y_pred_score[:, 1]))\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_valid, y_pred_score[:, 1])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same job with random forests\n",
    "my_model = ensemble.AdaBoostClassifier(base_estimator = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf = 50), \\\n",
    "                                       n_estimators=10)\n",
    "my_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "print(metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate probability score based ROC evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the validation data\n",
    "y_pred_score = my_model.predict_proba(X_valid)\n",
    "print(metrics.roc_auc_score(y_valid, y_pred_score[:, 1]))\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_valid, y_pred_score[:, 1])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do the same job with logistic regression\n",
    "my_model = linear_model.LogisticRegression()\n",
    "my_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the model on the **validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "print(metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate probability score based ROC evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the validation data\n",
    "y_pred_score = my_model.predict_proba(X_valid)\n",
    "print(metrics.roc_auc_score(y_valid, y_pred_score[:, 1]))\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_valid, y_pred_score[:, 1])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do the same job with random forests\n",
    "my_model = neighbors.KNeighborsClassifier()\n",
    "my_model = my_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the decision tree on the **validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "print(metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate probability score based ROC evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the validation data\n",
    "y_pred_score = my_model.predict_proba(X_valid)\n",
    "print(metrics.roc_auc_score(y_valid, y_pred_score[:, 1]))\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_valid, y_pred_score[:, 1])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do the same job with random forests\n",
    "my_model = XGBClassifier()\n",
    "my_model = my_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the decision tree on the **validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "print(metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate probability score based ROC evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the validation data\n",
    "y_pred_score = my_model.predict_proba(X_valid)\n",
    "print(metrics.roc_auc_score(y_valid, y_pred_score[:, 1]))\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_valid, y_pred_score[:, 1])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
